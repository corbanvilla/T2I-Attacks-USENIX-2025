{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from db import *\n",
    "from analysis import *\n",
    "from blackbox.helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit Guardrail: Prompt Softening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_moderation = aliased(ModerationRequest)\n",
    "revised_moderation = aliased(ModerationRequest)\n",
    "\n",
    "data = (\n",
    "    session.query(\n",
    "        Languages.name.label(\"Language\"),\n",
    "        func.avg(\n",
    "            (\n",
    "                (\n",
    "                    sql_norm(revised_moderation.category_scores)\n",
    "                    - sql_norm(original_moderation.category_scores)\n",
    "                )\n",
    "                / sql_norm(original_moderation.category_scores)\n",
    "            ) * 100\n",
    "        ).cast(Numeric(10,2)).label(\"Toxicity Change (%)\"),\n",
    "        func.avg(\n",
    "            sql_similarity(\n",
    "                original_moderation.category_scores, revised_moderation.category_scores\n",
    "            ) * 100\n",
    "        ).cast(Numeric(10,2)).label(\"Similarity (%)\"),\n",
    "    )\n",
    "    .select_from(ImageCreationRequest)\n",
    "    .join(\n",
    "        TranslatedPrompts,\n",
    "        ImageCreationRequest.translated_prompt_id == TranslatedPrompts.id,\n",
    "    )\n",
    "    .join(\n",
    "        Prompts,\n",
    "        TranslatedPrompts.prompt_id == Prompts.id,\n",
    "    )\n",
    "    .join(\n",
    "        Languages,\n",
    "        TranslatedPrompts.language_id == Languages.id,\n",
    "    )\n",
    "    .join(\n",
    "        original_moderation,\n",
    "        Prompts.prompt == original_moderation.prompt,\n",
    "    )\n",
    "    .join(\n",
    "        revised_moderation,\n",
    "        ImageCreationRequest.revised_prompt == revised_moderation.prompt,\n",
    "    )\n",
    "    .where(\n",
    "        ImageCreationRequest.model == \"dalle-3\",\n",
    "        ImageCreationRequest.success == True,\n",
    "        Languages.name != 'English',\n",
    "    )\n",
    "    .group_by(\n",
    "        Languages.name,\n",
    "    )\n",
    ")\n",
    "\n",
    "df = query_to_df(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "**Description:** This experiment quantifies the effect of the LLM prompt revision prompt as an implicit filtering mechanism in DALL·E 3 using two key metrics: *Toxicity Absolute Change* and *Toxicity Theme Similarity* (§3.3).\n",
    "\n",
    "**Dataset:** This experiment uses 5 prompts in 4 languages (Hawaiian, Lao, Nepali, Sinhala) for a total of 20 requests to DALL·E 3. Prompts and revised prompts are evaluated for toxicity using the OpenAI Moderation API.\n",
    "\n",
    "**Success Criteria:** The experiment demonstrates that the prompt revision model \"softens\" the toxicity of harmful prompts to various degrees of success under multilingual inputs:\n",
    "  - *Toxicity Absolute Change* will likely decrease for all languages, but not equally among all languages.\n",
    "  - *Toxicity Theme Similarity* will always decrease, but not equally among all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
